{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU\n",
      "load data successfully\n",
      "model size is  1.0x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nscc-gz-01/djs_FBIwarning/anaconda3/envs/py367_pytorch1.3.0/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10:11:51] TRAIN Iter 20: lr = 0.499967,\tloss = 9.745476,\tTop-1 err = 0.999219,\tTop-5 err = 0.993750,\tdata_time = 2.113861,\ttrain_time = 2.201211\n",
      "[10 10:12:37] TRAIN Iter 40: lr = 0.499933,\tloss = 6.922408,\tTop-1 err = 0.999023,\tTop-5 err = 0.994141,\tdata_time = 2.071388,\ttrain_time = 2.306452\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from network import ShuffleNetV1\n",
    "from utils import accuracy, AvgrageMeter, CrossEntropyLabelSmooth, save_checkpoint, get_lastest_model, get_parameters\n",
    "\n",
    "class OpencvResize(object):\n",
    "\n",
    "    def __init__(self, size=256):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        assert isinstance(img, PIL.Image.Image)\n",
    "        img = np.asarray(img) # (H,W,3) RGB\n",
    "        img = img[:,:,::-1] # 2 BGR\n",
    "        img = np.ascontiguousarray(img)\n",
    "        H, W, _ = img.shape\n",
    "        target_size = (int(self.size/H * W + 0.5), self.size) if H < W else (self.size, int(self.size/W * H + 0.5))\n",
    "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "        img = img[:,:,::-1] # 2 RGB\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = Image.fromarray(img)\n",
    "        return img\n",
    "\n",
    "class ToBGRTensor(object):\n",
    "\n",
    "    def __call__(self, img):\n",
    "        assert isinstance(img, (np.ndarray, PIL.Image.Image))\n",
    "        if isinstance(img, PIL.Image.Image):\n",
    "            img = np.asarray(img)\n",
    "        img = img[:,:,::-1] # 2 BGR\n",
    "        img = np.transpose(img, [2, 0, 1]) # 2 (3, H, W)\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        return img\n",
    "\n",
    "class DataIterator(object):\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = enumerate(self.dataloader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            _, data = next(self.iterator)\n",
    "        except Exception:\n",
    "            self.iterator = enumerate(self.dataloader)\n",
    "            _, data = next(self.iterator)\n",
    "        return data[0], data[1]\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\"ShuffleNetV1\")\n",
    "    parser.add_argument('--eval', default=False, action='store_true')\n",
    "    parser.add_argument('--eval-resume', type=str, default='./snet_detnas.pkl', help='path for eval model')\n",
    "    parser.add_argument('--batch-size', type=int, default=256, help='batch size')\n",
    "    parser.add_argument('--total-iters', type=int, default=300000, help='total iters')\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.5, help='init learning rate')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "    parser.add_argument('--weight-decay', type=float, default=4e-5, help='weight decay')\n",
    "    parser.add_argument('--save', type=str, default='./models', help='path for saving trained models')\n",
    "    parser.add_argument('--label-smooth', type=float, default=0.1, help='label smoothing')\n",
    "\n",
    "\n",
    "    parser.add_argument('--auto-continue', type=bool, default=True, help='auto continue')\n",
    "    parser.add_argument('--display-interval', type=int, default=20, help='display interval')\n",
    "    parser.add_argument('--val-interval', type=int, default=10000, help='val interval')\n",
    "    parser.add_argument('--save-interval', type=int, default=10000, help='save interval')\n",
    "\n",
    "\n",
    "    parser.add_argument('--group', type=int, default=3, help='group number')\n",
    "    parser.add_argument('--model-size', type=str, default='1.0x', choices=['0.5x', '1.0x', '1.5x', '2.0x'], help='size of the model')\n",
    "\n",
    "    parser.add_argument('--train-dir', type=str, default='/home/nscc-gz-01/djs_FBIwarning/ImageNet/raw-data/train', help='path to training dataset')\n",
    "    parser.add_argument('--val-dir', type=str, default='/home/nscc-gz-01/djs_FBIwarning/ImageNet/raw-data/val', help='path to validation dataset')\n",
    "\n",
    "    args = parser.parse_known_args()[0]#parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "\n",
    "    # Log\n",
    "    log_format = '[%(asctime)s] %(message)s'\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "        format=log_format, datefmt='%d %I:%M:%S')\n",
    "    t = time.time()\n",
    "    local_time = time.localtime(t)\n",
    "    if not os.path.exists('./log'):\n",
    "        os.mkdir('./log')\n",
    "    fh = logging.FileHandler(os.path.join('log/train-{}{:02}{}'.format(local_time.tm_year % 2000, local_time.tm_mon, t)))\n",
    "    fh.setFormatter(logging.Formatter(log_format))\n",
    "    logging.getLogger().addHandler(fh)\n",
    "\n",
    "    use_gpu = False\n",
    "    if torch.cuda.is_available():\n",
    "        use_gpu = True\n",
    "        print('Use GPU')\n",
    "\n",
    "    assert os.path.exists(args.train_dir)\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        args.train_dir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            ToBGRTensor(),\n",
    "        ])\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=10, pin_memory=use_gpu)\n",
    "    train_dataprovider = DataIterator(train_loader)\n",
    "\n",
    "    assert os.path.exists(args.val_dir)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(args.val_dir, transforms.Compose([\n",
    "            OpencvResize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            ToBGRTensor(),\n",
    "        ])),\n",
    "        batch_size=200, shuffle=False,\n",
    "        num_workers=1, pin_memory=use_gpu\n",
    "    )\n",
    "    val_dataprovider = DataIterator(val_loader)\n",
    "    print('load data successfully')\n",
    "\n",
    "    model = ShuffleNetV1(group=args.group, model_size=args.model_size)\n",
    "\n",
    "    optimizer = torch.optim.SGD(get_parameters(model),\n",
    "                                lr=args.learning_rate,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    criterion_smooth = CrossEntropyLabelSmooth(1000, 0.1)\n",
    "\n",
    "    if use_gpu:\n",
    "        #model = nn.DataParallel(model)\n",
    "        loss_function = criterion_smooth.cuda()\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        loss_function = criterion_smooth\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                    lambda step : (1.0-step/args.total_iters) if step <= args.total_iters else 0, last_epoch=-1)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    all_iters = 0\n",
    "    if args.auto_continue:\n",
    "        lastest_model, iters = get_lastest_model()\n",
    "        if lastest_model is not None:\n",
    "            all_iters = iters\n",
    "            checkpoint = torch.load(lastest_model, map_location=None if use_gpu else 'cpu')\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "            print('load from checkpoint')\n",
    "            for i in range(iters):\n",
    "                scheduler.step()\n",
    "\n",
    "    args.optimizer = optimizer\n",
    "    args.loss_function = loss_function\n",
    "    args.scheduler = scheduler\n",
    "    args.train_dataprovider = train_dataprovider\n",
    "    args.val_dataprovider = val_dataprovider\n",
    "\n",
    "    if args.eval:\n",
    "        if args.eval_resume is not None:\n",
    "            checkpoint = torch.load(args.eval_resume, map_location=None if use_gpu else 'cpu')\n",
    "            load_checkpoint(model, checkpoint)\n",
    "            validate(model, device, args, all_iters=all_iters)\n",
    "        exit(0)\n",
    "\n",
    "    while all_iters < args.total_iters:\n",
    "        all_iters = train(model, device, args, val_interval=args.val_interval, bn_process=False, all_iters=all_iters)\n",
    "        validate(model, device, args, all_iters=all_iters)\n",
    "    all_iters = train(model, device, args, val_interval=int(1280000/args.batch_size), bn_process=True, all_iters=all_iters)\n",
    "    validate(model, device, args, all_iters=all_iters)\n",
    "    save_checkpoint({'state_dict': model.state_dict(),}, args.total_iters, tag='bnps-')\n",
    "    torch.save(model.state_dict(), 'model.mdl')\n",
    "\n",
    "def adjust_bn_momentum(model, iters):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.momentum = 1 / iters\n",
    "\n",
    "def train(model, device, args, *, val_interval, bn_process=False, all_iters=None):\n",
    "\n",
    "    optimizer = args.optimizer\n",
    "    loss_function = args.loss_function\n",
    "    scheduler = args.scheduler\n",
    "    train_dataprovider = args.train_dataprovider\n",
    "\n",
    "    t1 = time.time()\n",
    "    Top1_err, Top5_err = 0.0, 0.0\n",
    "    model.train()\n",
    "    for iters in range(1, val_interval + 1):\n",
    "        scheduler.step()\n",
    "        if bn_process:\n",
    "            adjust_bn_momentum(model, iters)\n",
    "\n",
    "        all_iters += 1\n",
    "        d_st = time.time()\n",
    "        data, target = train_dataprovider.next()\n",
    "        target = target.type(torch.LongTensor)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data_time = time.time() - d_st\n",
    "\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "        Top1_err += 1 - prec1.item() / 100\n",
    "        Top5_err += 1 - prec5.item() / 100\n",
    "\n",
    "        if all_iters % args.display_interval == 0:\n",
    "            printInfo = 'TRAIN Iter {}: lr = {:.6f},\\tloss = {:.6f},\\t'.format(all_iters, scheduler.get_lr()[0], loss.item()) + \\\n",
    "                        'Top-1 err = {:.6f},\\t'.format(Top1_err / args.display_interval) + \\\n",
    "                        'Top-5 err = {:.6f},\\t'.format(Top5_err / args.display_interval) + \\\n",
    "                        'data_time = {:.6f},\\ttrain_time = {:.6f}'.format(data_time, (time.time() - t1) / args.display_interval)\n",
    "            logging.info(printInfo)\n",
    "            t1 = time.time()\n",
    "            Top1_err, Top5_err = 0.0, 0.0\n",
    "\n",
    "        if all_iters % args.save_interval == 0:\n",
    "            save_checkpoint({\n",
    "                'state_dict': model.state_dict(),\n",
    "                }, all_iters)\n",
    "\n",
    "    return all_iters\n",
    "\n",
    "def validate(model, device, args, *, all_iters=None):\n",
    "    objs = AvgrageMeter()\n",
    "    top1 = AvgrageMeter()\n",
    "    top5 = AvgrageMeter()\n",
    "\n",
    "    loss_function = args.loss_function\n",
    "    val_dataprovider = args.val_dataprovider\n",
    "\n",
    "    model.eval()\n",
    "    max_val_iters = 250\n",
    "    t1  = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(1, max_val_iters + 1):\n",
    "            data, target = val_dataprovider.next()\n",
    "            target = target.type(torch.LongTensor)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "\n",
    "            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "            n = data.size(0)\n",
    "            objs.update(loss.item(), n)\n",
    "            top1.update(prec1.item(), n)\n",
    "            top5.update(prec5.item(), n)\n",
    "\n",
    "    logInfo = 'TEST Iter {}: loss = {:.6f},\\t'.format(all_iters, objs.avg) + \\\n",
    "              'Top-1 err = {:.6f},\\t'.format(1 - top1.avg / 100) + \\\n",
    "              'Top-5 err = {:.6f},\\t'.format(1 - top5.avg / 100) + \\\n",
    "              'val_time = {:.6f}'.format(time.time() - t1)\n",
    "    logging.info(logInfo)\n",
    "\n",
    "def load_checkpoint(net, checkpoint):\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    temp = OrderedDict()\n",
    "    if 'state_dict' in checkpoint:\n",
    "        checkpoint = dict(checkpoint['state_dict'])\n",
    "    for k in checkpoint:\n",
    "        k2 = 'module.'+k if not k.startswith('module.') else k\n",
    "        temp[k2] = checkpoint[k]\n",
    "\n",
    "    net.load_state_dict(temp, strict=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
